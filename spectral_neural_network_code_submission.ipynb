{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPZWfVHI3R6S"
   },
   "outputs": [],
   "source": [
    "# Essential\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as Transform\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import scipy\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "# Display support\n",
    "import cv2\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = 'monospace' # windows\n",
    "# mpl.rcParams['font.family'] = 'Helvetica' # iOS\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['figure.dpi'] = 350\n",
    "mpl.rcParams[\"text.usetex\"]\n",
    "colors = ['C1', 'C6', 'C2', 'C9', 'C10', 'C3', 'C4', 'C7', 'C8', 'C5']*10\n",
    "markers = [r'$\\bigcirc$', r'$\\boxdot$', r'$\\bigtriangleup$', r'$\\heartsuit$', r'$\\diamondsuit$', 'v']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8P_jgZr3ZEJ"
   },
   "outputs": [],
   "source": [
    "def dot(A, X):\n",
    "    m = A.shape[0]\n",
    "    X = X.repeat(m, 1, 1)\n",
    "    AX = torch.sum(A * X, dim=[1, 2])\n",
    "    return AX\n",
    "\n",
    "def spectral_init(n, Phi, PsiT): # Assumption 2 in our paper\n",
    "    if False:\n",
    "        G = torch.tensor(ortho_group.rvs(n), dtype=torch.float) # kills kernel\n",
    "    else:\n",
    "        Gtemp = torch.rand(n,n)\n",
    "        Gtemp1, _, Gtemp2 = torch.linalg.svd(Gtemp)\n",
    "        G = Gtemp1 #@Gtemp2\n",
    "    Ubar, _ = torch.sort(torch.rand(n))\n",
    "    Vbar, _ = torch.sort(torch.rand(n))\n",
    "    U_init = Phi @ torch.diag(Ubar) @ G\n",
    "    V_init = PsiT.T @ torch.diag(Vbar) @ G\n",
    "    return U_init, V_init\n",
    "\n",
    "def spectral_nonlinear(Q):\n",
    "    # Apply a non-linear on the spectrum of Q\n",
    "    U, S, Vt = torch.linalg.svd(Q, full_matrices=False)\n",
    "    S = torch.diag_embed(torch.nn.functional.tanh(S))\n",
    "    USVt = torch.bmm(torch.bmm(U, S), Vt)\n",
    "    return USVt\n",
    "\n",
    "def mse_loss(X, A, y):\n",
    "    return 0.5 * torch.mean(torch.square(dot(A, X) - y))\n",
    "\n",
    "def test_error(X, Xtrue):\n",
    "    return torch.norm(X - Xtrue, p='fro') / torch.norm(Xtrue, p='fro')\n",
    "\n",
    "def erank(X):\n",
    "    S = torch.linalg.svdvals(X)\n",
    "    p = torch.div(S,torch.sum(S))\n",
    "    efr = torch.exp(-torch.sum(torch.mul(p,torch.log(p))))\n",
    "    return efr\n",
    "def get_norm_grad(model):\n",
    "    try:\n",
    "        grad_norm = 0\n",
    "        for param in model.parameters():\n",
    "            grad_norm = grad_norm + torch.square(torch.norm(param.grad, p=2))\n",
    "        grad_norm = torch.sqrt(grad_norm)\n",
    "    except:\n",
    "        grad_norm = torch.tensor(float('inf'))\n",
    "    return grad_norm\n",
    "\n",
    "def PSNR(original, compressed):\n",
    "    mse = torch.mean(torch.square(original - compressed)).detach().numpy()\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A1T0rk_3iFo"
   },
   "outputs": [],
   "source": [
    "def plot_matrix(inputmat, inputaxes = None, title = None):\n",
    "    if inputaxes is None:\n",
    "        figout, axesout = plt.subplots(nrows=1, ncols=1,figsize=(2,1.5), linewidth=10, edgecolor=\"#04253a\")\n",
    "    else:\n",
    "        axesout = inputaxes\n",
    "    try:\n",
    "        inputmat = torch.clamp(inputmat, min=0, max=255)\n",
    "        inputmat = inputmat.detach().numpy().astype('uint8')\n",
    "        axesout.imshow(cv2.cvtColor(inputmat, cv2.COLOR_BGR2RGB))\n",
    "        axesout.axis(\"off\")\n",
    "        axesout.title.set_text(title)\n",
    "    except:\n",
    "        print(\"An error in ploting matrix\")\n",
    "    return\n",
    "def print_output(X,A_train,y_train,A_test,y_test,Xtrue):\n",
    "    nuc_norm = torch.norm(X,p='nuc')\n",
    "    print(f'nuclear norm: {nuc_norm:.2f}, erank: {erank(X):.2f}, rank: {torch.linalg.matrix_rank(X):.2f}')\n",
    "    print(f'train loss: {mse_loss(X,A_train,y_train):.2e}, test loss: {mse_loss(X,A_test,y_test):.2e}')\n",
    "    print(f'test error <recovering error>: {test_error(X,Xtrue):.2e}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfbt4sKt3gR3"
   },
   "outputs": [],
   "source": [
    "def run_gradient_descent(model, A_train, y_train, A_test, y_test,\n",
    "                         lr=1e-4, num_iter=10000, repeat = 1,\n",
    "                         freq=5, batch_size = None, momentum=0,\n",
    "                         Xtrue= None, scheduler_flag=False):\n",
    "\n",
    "    target_best = float('inf')\n",
    "    target_traces = []\n",
    "    model_org = copy.deepcopy(model)\n",
    "    if batch_size is None:\n",
    "        batch_size = A_train.shape[0]\n",
    "    if lr == 'search':\n",
    "        lr_list = [ ii*10**rate for rate in range(1,-12,-1) for ii in [7.5,5.0,2.5,1.0]]\n",
    "        return_tag = True\n",
    "    elif type(lr) is list:\n",
    "        lr_list = lr\n",
    "        return_tag = False\n",
    "    else:\n",
    "        lr_list = [lr]\n",
    "        return_tag = False\n",
    "\n",
    "\n",
    "    for rep in (mbar:= master_bar(range(repeat*len(lr_list)))):\n",
    "        lr = lr_list[rep//repeat]\n",
    "        try:\n",
    "            model = copy.deepcopy(model_org)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,num_iter//111)\n",
    "            train_loss_traces, test_loss_traces, test_error_traces = [], [], []\n",
    "            singular_value_traces, nuclear_norm_traces, erank_traces = [],[],[]\n",
    "            grad_traces, lr_traces, top_sv_traces, bot_sv_traces, X_traces, iter_traces = [], [], [], [], [], []\n",
    "            batch_loader = DataLoader(torch.utils.data.TensorDataset(A_train, y_train),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "            # train\n",
    "            model.train()\n",
    "            for i in (pbar:= progress_bar(range(num_iter), parent=mbar)):\n",
    "                (A_batch,y_batch) = next(iter(batch_loader))\n",
    "                X_temp = model()\n",
    "                loss = mse_loss(X_temp,A_batch,y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler_flag:\n",
    "                    scheduler.step()\n",
    "                loss_train = mse_loss(X_temp, A_train, y_train)\n",
    "                if torch.isnan(loss_train) or torch.isinf(loss_train):\n",
    "                    raise Exception('numerical error')\n",
    "                # record\n",
    "                if i % freq == 0:\n",
    "                    iter_traces.append(i)\n",
    "                    train_loss_traces.append(loss_train.detach().numpy())\n",
    "                    loss_test = mse_loss(X_temp, A_test, y_test)\n",
    "                    test_loss_traces.append(loss_test.detach().numpy())\n",
    "\n",
    "\n",
    "                    nucnorm = torch.norm(X_temp,p='nuc').detach().numpy()\n",
    "                    svs = torch.linalg.svdvals(X_temp).detach().numpy()\n",
    "                    top_sv_traces.append(sorted(svs)[-3:])\n",
    "                    bot_sv_traces.append(sorted(svs)[:2])\n",
    "                    singular_value_traces.append(svs)\n",
    "                    nuclear_norm_traces.append(nucnorm)\n",
    "\n",
    "                    erank_value = erank(X_temp)\n",
    "                    erank_traces.append(erank_value.detach().numpy())\n",
    "                    X_traces.append(X_temp)\n",
    "                    grad_traces.append(get_norm_grad(model).detach().numpy())\n",
    "                    cur_lr = scheduler.get_last_lr()[0]\n",
    "                    lr_traces.append(cur_lr)\n",
    "                    try:\n",
    "                        test_error_val = test_error(X_temp,Xtrue).detach().numpy()\n",
    "                    except:\n",
    "                        test_error_val = 1.0\n",
    "                    test_error_traces.append(test_error_val)\n",
    "                    # record\n",
    "                    traces = {  '_': iter_traces,\n",
    "                        'train_loss_traces': train_loss_traces,\n",
    "                        'test_loss_traces': test_loss_traces,\n",
    "                        'test_error_traces': test_error_traces,\n",
    "                        'erank_traces': erank_traces,\n",
    "                        'nuclear_norm_traces': nuclear_norm_traces,\n",
    "                        'top_sv_traces': top_sv_traces,\n",
    "                        'bot_sv_traces': bot_sv_traces,\n",
    "                        'grad_traces': grad_traces,\n",
    "                        'lr_traces': lr_traces,\n",
    "                        'singular_value_traces': singular_value_traces,\n",
    "                        'X_traces': X_traces  }\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print\n",
    "                mbar.child.comment = f' ] [train {loss_train:.2e}, nuc-nrm {nucnorm:.2f}, lr {cur_lr:.2e}, test err {test_error_val:.2e}'\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            target_val = loss_train.detach().numpy()\n",
    "            success_flag = True\n",
    "        except Exception as emes:\n",
    "#             print(emes)\n",
    "            target_val = float('inf')\n",
    "            success_flag = False\n",
    "            pass\n",
    "        # Compare\n",
    "        target_traces.append(target_val)\n",
    "        if success_flag and (target_val < target_best or rep == 0):\n",
    "            model_best = copy.deepcopy(model)\n",
    "            target_best = target_val\n",
    "            traces_best = traces\n",
    "            lr_best = lr\n",
    "            # print\n",
    "            mbar.main_bar.comment = f'] [best train {target_val:.2e} <{lr_best:.2e}>'\n",
    "        elif success_flag and return_tag and target_val > target_best:\n",
    "            return traces_best, model_best(), model_best, lr_best, target_traces\n",
    "    return traces_best, model_best(), model_best, lr_best, target_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDyy7OgR3bNL"
   },
   "outputs": [],
   "source": [
    "class ParamsInit(torch.nn.Module):\n",
    "    def __init__(self, n, in_dim=[1], init_tag = None, Phi=None, PsiT=None):\n",
    "        super(ParamsInit, self).__init__()\n",
    "        self.n = n\n",
    "        self.first_dim = in_dim[0]\n",
    "        self.all_dim = in_dim + [1]\n",
    "        self.n_layers = len(in_dim)\n",
    "        self.Us = torch.nn.ParameterList()\n",
    "        self.Vs = torch.nn.ParameterList()\n",
    "        self.alphas = torch.nn.ParameterList()\n",
    "        self.init_tag = init_tag\n",
    "        # Initialize bias\n",
    "        self.bias = torch.nn.parameter.Parameter(torch.zeros(n,n))\n",
    "        # Initialize Us and Vs\n",
    "        for i in range(self.first_dim):\n",
    "            if init_tag == 'well_spec':\n",
    "                U, V = spectral_init(n, Phi, PsiT)\n",
    "            elif init_tag == 'random':\n",
    "                U = torch.rand(n,n)\n",
    "                V = torch.rand(n,n)\n",
    "            elif init_tag == 'diag':\n",
    "                U = torch.diag(torch.rand(n))\n",
    "                V = torch.diag(torch.rand(n))\n",
    "            elif init_tag == 'identity':\n",
    "                eps = 10**-4\n",
    "                U, V = torch.eye(n)*eps,torch.eye(n)*eps\n",
    "            U = torch.nn.Parameter(U)\n",
    "            V = torch.nn.Parameter(V)\n",
    "            self.Us.append(U)\n",
    "            self.Vs.append(V)\n",
    "        # Initialize alphas\n",
    "        for i in range(self.n_layers):\n",
    "            alpha_i = torch.nn.Parameter(torch.rand(self.all_dim[i],self.all_dim[i+1]))\n",
    "            self.alphas.append(alpha_i)\n",
    "class SNN(torch.nn.Module):\n",
    "    def __init__(self, model_ref=None):\n",
    "        super(SNN, self).__init__()\n",
    "        self.n = model_ref.n\n",
    "        self.first_dim = model_ref.first_dim\n",
    "        self.all_dim = model_ref.all_dim\n",
    "        self.model_name = 'snn_' + model_ref.init_tag+'_'+str(self.all_dim)\n",
    "        self.n_layers = model_ref.n_layers\n",
    "        self.Us = model_ref.Us\n",
    "        self.Vs = model_ref.Vs\n",
    "        if model_ref.init_tag == 'identity':\n",
    "            for i, (U,V) in enumerate(zip(self.Us,self.Vs)):\n",
    "                self.Us[i] = U + torch.diag_embed(1e-6*torch.randn(self.n))\n",
    "                self.Vs[i] = V + torch.diag_embed(1e-6*torch.randn(self.n))\n",
    "\n",
    "        self.alphas = model_ref.alphas\n",
    "    def forward(self):\n",
    "        X = [torch.matmul(U , V.T) for U, V in zip(self.Us, self.Vs)]\n",
    "        X = torch.stack(X, axis=0)\n",
    "        for i in range(self.n_layers):\n",
    "            X = spectral_nonlinear(X) \n",
    "            X_T = torch.permute(X,(1,2,0))\n",
    "            X = torch.permute(torch.matmul(X_T,self.alphas[i]), (2,0,1))\n",
    "        X = torch.squeeze(X)\n",
    "        return X\n",
    "class WOA(torch.nn.Module):\n",
    "    def __init__(self, model_ref=None):\n",
    "        super(WOA, self).__init__()\n",
    "        self.model_name = 'woa_' + model_ref.init_tag +'_D'+str(self.first_dim)\n",
    "        self.n = model_ref.n\n",
    "        self.first_dim = model_ref.first_dim\n",
    "        self.all_dim = model_ref.all_dim\n",
    "        self.n_layers = model_ref.n_layers\n",
    "        self.Us = model_ref.Us\n",
    "        self.Vs = model_ref.Vs\n",
    "        self.alphas = model_ref.alphas\n",
    "    def forward(self):\n",
    "        X = [torch.matmul(U , V.T) for U, V in zip(self.Us, self.Vs)]\n",
    "        X = torch.stack(X, axis=0)\n",
    "        for i in range(self.n_layers):\n",
    "            X_T = torch.permute(X,(1,2,0))\n",
    "            X = torch.permute(torch.matmul(X_T,self.alphas[i]), (2,0,1))\n",
    "        X = torch.squeeze(X)\n",
    "        return X\n",
    "class DF(torch.nn.Module):\n",
    "    def __init__(self, model_ref=None, depth=None):\n",
    "        super(DF, self).__init__()\n",
    "        self.n = model_ref.n\n",
    "        self.first_dim = model_ref.first_dim\n",
    "        Us = model_ref.Us\n",
    "        Vs = model_ref.Vs\n",
    "        if depth is None:\n",
    "            self.depth = self.first_dim * 2\n",
    "        else:\n",
    "            self.depth = depth\n",
    "        self.model_name = 'df_' + model_ref.init_tag+'_'+str(self.depth)\n",
    "\n",
    "        self.Ws = torch.nn.ParameterList()\n",
    "        for i in range(self.first_dim):\n",
    "            self.Ws.append(torch.nn.Parameter(Us[i]))\n",
    "            self.Ws.append(torch.nn.Parameter(torch.transpose(Vs[i],0,1)))\n",
    "        self.Ws = self.Ws[:self.depth]\n",
    "    def forward(self):\n",
    "        X = torch.eye(self.n)\n",
    "        for i in range(self.depth):\n",
    "            X = torch.matmul(X,self.Ws[i])\n",
    "        X = torch.squeeze(X)\n",
    "        return X\n",
    "\n",
    "class LG(torch.nn.Module):\n",
    "    def __init__(self, model_ref=None):\n",
    "        super(LG, self).__init__()\n",
    "        self.model_name = 'lg'\n",
    "        self.n = model_ref.n\n",
    "        self.W = torch.nn.Parameter(torch.zeros(self.n,self.n))\n",
    "    def forward(self):\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DKckXyK3krZ"
   },
   "outputs": [],
   "source": [
    "def assign_subtensor(bigmat,row_idx,col_idx,submat):\n",
    "    for smallr,bigr in enumerate(row_idx):\n",
    "        for smallc,bigc in enumerate(col_idx):\n",
    "            bigmat[bigr,bigc] = submat[smallr,smallc]\n",
    "    return bigmat\n",
    "\n",
    "def generate_X(n, Xlabel=0, nlow=None, Xtrue = None, image_path = None, prescale=True):\n",
    "    if Xlabel == 'synthetic': # random, PSD\n",
    "        if nlow is None:\n",
    "            nlow = n\n",
    "        Utemp = torch.rand(n,nlow)\n",
    "        Xtrue = Utemp @ Utemp.T\n",
    "        Xtrue = Xtrue * 100.0\n",
    "    elif Xlabel == 'predefined': # pre-defined Xtrue\n",
    "        Xtrue = torch.from_numpy(Xtrue).float()\n",
    "        Xtrue_unsq = Xtrue[None, :, :]\n",
    "        avg_fun = Transform.Resize((n,n))\n",
    "        Xtrue = avg_fun(Xtrue_unsq).squeeze()\n",
    "\n",
    "    # Scale\n",
    "    if prescale:\n",
    "        X_scale = torch.norm(Xtrue,p='nuc')\n",
    "    else:\n",
    "        X_scale = 1\n",
    "    Xtrue /= X_scale\n",
    "    Phi, X_svd_vec, PsiT = torch.linalg.svd(Xtrue)\n",
    "\n",
    "    return  Xtrue, X_scale, Phi, PsiT, X_svd_vec\n",
    "\n",
    "\n",
    "def generate_A(Xtrue, Phi=None, PsiT=None, m=10, Alabel=0, noise=True):\n",
    "\n",
    "    n = Xtrue.shape[0]\n",
    "\n",
    "    A = []\n",
    "    A_svd_vec = []\n",
    "    for i in (pbar:= progress_bar(range(2*m))):\n",
    "        if Alabel == 'well_spec':\n",
    "            A_svd_vec_i = torch.randn(n)*n\n",
    "            A_svd_vec_i, _ = torch.sort(A_svd_vec_i, descending=True)\n",
    "            Ai = Phi @ torch.diag(A_svd_vec_i) @ PsiT\n",
    "        elif Alabel == 'mis_spec':\n",
    "            Ai = torch.randn(n, n)*n\n",
    "\n",
    "\n",
    "        A.append(Ai)\n",
    "        A_svd_vec_i = torch.linalg.svdvals(Ai)\n",
    "        A_svd_vec.append(A_svd_vec_i)\n",
    "\n",
    "    A = torch.stack(A)\n",
    "    y = dot(A, Xtrue)\n",
    "    if noise:\n",
    "        y = y + torch.randn(2*m)*1e-2\n",
    "\n",
    "    A_train = A[:m]\n",
    "    y_train = y[:m]\n",
    "    A_test  = A[m:]\n",
    "    y_test  = y[m:]\n",
    "    return A_train, y_train, A_test, y_test, A_svd_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C61c1VOR3ozQ"
   },
   "source": [
    "# Generate dataset & built networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VDiDm_cuybS2",
    "outputId": "048d55ae-1b48-465b-8882-9fa9fdce1abe"
   },
   "outputs": [],
   "source": [
    "'''Choose one of the following line'''\n",
    "# Example, Assumption, Noisy, Figure = 'syn', 'well_spec', True, 'Figure1' # Figure 1\n",
    "# Example, Assumption, Noisy, Figure = 'syn', 'mis_spec', True, 'Figure2' # Figure 2\n",
    "Example, Assumption, Noisy, Figure, Findex = 'dgt', 'mis_spec', True, 'Figure3', 0 # Figure 3\n",
    "'''--------------------------------------------------------------------------'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Generate X\n",
    "    if Example == 'syn':\n",
    "        n, nlow = 10, 6\n",
    "        Xtrue, X_scale, Phi, PsiT, _ = generate_X(n=n, Xlabel='synthetic', nlow=nlow)\n",
    "        nlow = torch.linalg.matrix_rank(Xtrue)\n",
    "        if Figure == 'Figure1':            \n",
    "            num_iter = 100001\n",
    "        else:\n",
    "            num_iter = 10001\n",
    "        m = 75\n",
    "        batch_size = m\n",
    "        num_lr = 5\n",
    "    elif Example == 'dgt':\n",
    "        from keras.datasets import mnist, fashion_mnist\n",
    "        (Xmnist, _), (_, _) = mnist.load_data()\n",
    "        X_dgt = Xmnist[Findex,:,:]\n",
    "        n = X_dgt.shape[0]\n",
    "        Xtrue, X_scale, Phi, PsiT, _ = generate_X(n=n, Xlabel='predefined', Xtrue=X_dgt)\n",
    "        num_iter = 2501\n",
    "        m = round(n*n*0.6)\n",
    "        batch_size = m\n",
    "        num_lr = 3\n",
    "\n",
    "    # Generate A\n",
    "    if Assumption == 'well_spec':\n",
    "        A_train, y_train, A_test, y_test, _ = generate_A(Xtrue, Phi=Phi, PsiT=PsiT, m=m, Alabel='well_spec', noise=Noisy)\n",
    "    elif Assumption == 'mis_spec':\n",
    "        Phi, PsiT = None, None\n",
    "        A_train, y_train, A_test, y_test, _ = generate_A(Xtrue, m=m, Alabel='mis_spec', noise=Noisy)\n",
    "\n",
    "\n",
    "    # Built networks\n",
    "    if Figure == 'Figure1' or Figure == 'Figure2':\n",
    "        in_dim = [2,2,2,2]\n",
    "    else:\n",
    "        in_dim = [4,8,16]\n",
    "\n",
    "    \n",
    "\n",
    "    if Figure == 'Figure1':\n",
    "        params_init_w = ParamsInit(n, in_dim=in_dim, Phi=Phi, PsiT=PsiT, init_tag='well_spec') # well spec\n",
    "        model_snn_w = SNN(params_init_w)\n",
    "        list_model_to_run = [model_snn_w]\n",
    "    elif Figure == 'Figure2':\n",
    "        params_init_i = ParamsInit(n, in_dim=in_dim, Phi=None, PsiT=None, init_tag='identity') # identity\n",
    "        model_snn_i = SNN(params_init_i)\n",
    "        \n",
    "        model_df_i3 = DF(params_init_i, depth=3)\n",
    "        \n",
    "        model_lg = LG(params_init_i)\n",
    "        list_model_to_run = [model_snn_i,model_lg,model_df_i3]\n",
    "    elif Figure == 'Figure3':\n",
    "        params_init_i = ParamsInit(n, in_dim=in_dim, Phi=None, PsiT=None, init_tag='identity') # identity\n",
    "        model_snn_i = SNN(params_init_i)\n",
    "        \n",
    "        model_lg = LG(params_init_i)\n",
    "        list_model_to_run = [model_lg,model_snn_i]\n",
    "\n",
    "    # Display\n",
    "    print('+'*100)\n",
    "    nucnorm = torch.norm(Xtrue,p='nuc')\n",
    "    print(f'n = {n}, nlow = {torch.linalg.matrix_rank(Xtrue)}, erank = {erank(Xtrue):.2f}, nuclear norm: {nucnorm:.2f}'\n",
    "          f'\\nm {m}, batch {batch_size}'\n",
    "          f'\\ntrain loss: {mse_loss(Xtrue,A_train,y_train):.2e}, test loss: {mse_loss(Xtrue,A_test,y_test):.2e}'\n",
    "          f'\\nPhi {Phi}, \\nPsiT {PsiT} \\nXtrue {Xtrue}\\nAi {A_train[0]}\\nyi {y_train[0]}')\n",
    "    plot_matrix(Xtrue*X_scale, title='ground-truth')\n",
    "    for model in list_model_to_run:\n",
    "        print('+'*100)\n",
    "        print(f'model {model.model_name}: no of params {count_parameters(model)}\\ninitalization X {model()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVjR837UybS4"
   },
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "7NvAxJEyybS4",
    "outputId": "71e840cb-093e-4e2a-bfca-99bb298f875a"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    problem_id = Example+'_'+Assumption+'_'+str(Noisy)\n",
    "    model_names = [model.model_name for model in list_model_to_run]\n",
    "    lrs = [ ii*10**rate for rate in range(1,-12,-1) for ii in [5.0,1.0]]\n",
    "\n",
    "\n",
    "    print(f'problem id {problem_id}\\nm = {m}, n = {n}, iter = {num_iter}, batch = {batch_size}'\n",
    "          f'\\nmodel: {model_names}\\nlrs: {lrs}')\n",
    "\n",
    "    Outputs = {'problem_id':problem_id,\n",
    "#                'params_init':[params_init_r,params_init_i],\n",
    "               'list_model': list_model_to_run,\n",
    "               'model_names': model_names,\n",
    "               'Xtrue': Xtrue,\n",
    "               'X_scale': X_scale,\n",
    "              'A_train': A_train,\n",
    "              'y_train': y_train,\n",
    "              'A_test': A_test,\n",
    "              'y_test': y_test,\n",
    "#               'in_dim': in_dim\n",
    "              }\n",
    "\n",
    "    for m_count, model in enumerate(list_model_to_run):        \n",
    "        key_out = model.model_name\n",
    "        print('\\n\\n\\n'+'+'*100+'\\n')\n",
    "\n",
    "        results = {}\n",
    "        lr_count=0\n",
    "\n",
    "\n",
    "        if 'df' in key_out:\n",
    "            factor = 2\n",
    "        else:\n",
    "            factor = 1\n",
    "\n",
    "\n",
    "        for lr in lrs:\n",
    "            if lr_count <  num_lr*math.ceil(factor):\n",
    "                print(f'model {key_out}, lr = {lr:.2e}, {m_count} of {len(list_model_to_run)} models')\n",
    "                try:\n",
    "                    model_in = copy.deepcopy(model)\n",
    "                    traces_out, X_out, model_out, lr_out, target_traces = run_gradient_descent(\n",
    "                        model_in, A_train, y_train, A_test, y_test,\n",
    "                        lr=lr, num_iter=round(num_iter*factor),\n",
    "                        freq=round(num_iter*factor)//100, batch_size=batch_size,\n",
    "                        repeat = 1, Xtrue = Xtrue)\n",
    "                    results[lr] = traces_out\n",
    "                    print_output(X_out,A_train,y_train,A_test,y_test,Xtrue)\n",
    "                    lr_count += 1\n",
    "                except Exception as emes:\n",
    "                    print(emes)\n",
    "                print('-'*100)\n",
    "\n",
    "                # save\n",
    "                Outputs[key_out] = results\n",
    "                \n",
    "                if Figure=='Figure3':\n",
    "                    file_save_name = 'output/'+Figure+'_'+str(m)+'_'+str(Findex)+'_'+problem_id+'.pkl', 'wb'\n",
    "                else:\n",
    "                    file_save_name = 'output/'+Figure+'_'+str(m)+'_'+problem_id+'.pkl', 'wb'\n",
    "                with open('output/'+Figure+'_'+str(m)+problem_id+'.pkl', 'wb') as file:\n",
    "                    pickle.dump(Outputs, file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
